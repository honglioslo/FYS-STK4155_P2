{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done producing Ising data\n",
      "----------------congratuations----------------\n"
     ]
    }
   ],
   "source": [
    "# a) producing data for Ising model\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40\n",
    "# create n_sample = 10000 random Ising states\n",
    "n_sample=10000\n",
    "states_ori=np.random.choice([-1,1], size=(n_sample,L))\n",
    "\n",
    "# define functions\n",
    "\n",
    "def ising_energies(states,L):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    J=np.zeros((L,L),)\n",
    "    for i in range(L):\n",
    "        J[i,(i+1)%L]-=1.0\n",
    "    \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "    \n",
    "    return E\n",
    "\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states_ori,L)\n",
    "print(\"done producing Ising data\")\n",
    "print(\"----------------congratuations----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error for OLS 0.0000 \n",
      "                R2 for OLS 1.0000 \n",
      "mean squared error for Ridge 0.0000 \n",
      "                R2 for Ridge 1.0000 \n",
      "mean squared error for Lasso 0.0041 \n",
      "                R2 for Lasso 0.9999 \n",
      "done regression\n",
      "----------------congratuations----------------\n"
     ]
    }
   ],
   "source": [
    "# b) Estimating the coupling constant of the one-dimensional Ising model using linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# making regression data and split training and test data\n",
    "\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states_ori, states_ori)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "\n",
    "#states=np.einsum('...i,...j->...ij', states, states)\n",
    "#shape=states.shape\n",
    "#states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "X_train, X_test, y_train, y_test = train_test_split(states, energies, test_size=0.25)\n",
    "# set up Lasso and Ridge Regression models, from we set par = 0.01\n",
    "reg_par=0.01\n",
    "\n",
    "# perform regressions\n",
    "# OLS\n",
    "OLS=linear_model.LinearRegression().fit(X_train, y_train) # fit model \n",
    "print(\"mean squared error for OLS %.4f \" % mean_squared_error(y_test, OLS.predict(X_test)))\n",
    "print(\"                R2 for OLS %.4f \" % r2_score(y_test, OLS.predict(X_test)))\n",
    "\n",
    "# Ridge\n",
    "ridge=linear_model.Ridge(alpha=reg_par)\n",
    "ridge.fit(X_train, y_train) \n",
    "print(\"mean squared error for Ridge %.4f \" % mean_squared_error(y_test, ridge.predict(X_test)))\n",
    "print(\"                R2 for Ridge %.4f \" % r2_score(y_test, ridge.predict(X_test)))\n",
    "\n",
    "# Lasso\n",
    "lasso = linear_model.Lasso(alpha=reg_par)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"mean squared error for Lasso %.4f \" % mean_squared_error(y_test, lasso.predict(X_test)))\n",
    "print(\"                R2 for Lasso %.4f \" % r2_score(y_test, lasso.predict(X_test)))\n",
    "\n",
    "\n",
    "print(\"done regression\")\n",
    "print(\"----------------congratuations----------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Determine the phase of the two dimensional Ising model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed() # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit\n",
    "\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.75 # training samples\n",
    "\n",
    "# load data\n",
    "path_to_data='C:/Users/honli/Documents/GitHub/FYS-STK4155_P2/data'\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "data = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=0.25)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "\n",
    "###### apply logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# define regularisation parameter\n",
    "lmbdas=0.1\n",
    "\n",
    "# define logistic regressor\n",
    "logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5)\n",
    "\n",
    "# fit training data\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "# define Stochastic Gradient Descent logistic regression\n",
    "logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
    "                                        shuffle=True, random_state=1, learning_rate='optimal')\n",
    "# fit training data\n",
    "logreg_SGD.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) regression analysis of the one dimensional Ising model using neural networks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# \n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "def feed_forward_train(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    # for backpropagation need activations in hidden and output layers\n",
    "    return a_h, probabilities\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a_h, probabilities = feed_forward_train(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = probabilities - Y\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "\n",
    "def to_categorical_numpy(integer_vector):    \n",
    "    n_inputs = len(integer_vector)\n",
    "    unique_values = np.array(list(set(integer_vector)))\n",
    "    n_categories = len(unique_values)\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    for iI in range(n_inputs):\n",
    "        onehot_vector[iI, int(np.asarray(np.where(unique_values == integer_vector[iI])))] = 1\n",
    "        \n",
    "    return onehot_vector\n",
    "\n",
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    probabilities = feed_forward(X)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "n_hidden_neurons = 50\n",
    "\n",
    "states=np.einsum('...i,...j->...ij', states_ori, states_ori)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "Y_onehot = to_categorical_numpy(energies)\n",
    "\n",
    "X_train, X_test, Y_train_onehot, Y_test_onehot = train_test_split(states, Y_onehot, test_size=0.25)\n",
    "n_inputs, n_features = X_train.shape\n",
    "\n",
    "n_categories = Y_onehot.shape[1]\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "probabilities = feed_forward(X_train)\n",
    "#print(\"probabilities = (n_inputs, n_categories) = \" + str(probabilities.shape))\n",
    "#print(\"probability that image 0 is in category 0,1,2,...,9 = \\n\" + str(probabilities[0]))\n",
    "#print(\"probabilities sum up to: \" + str(probabilities[0].sum()))\n",
    "#print()\n",
    "\n",
    "\n",
    "predictions = predict(X_train)\n",
    "\n",
    "eta = 0.01\n",
    "lmbd = 0.01\n",
    "for i in range(1000):\n",
    "    # calculate gradients\n",
    "    dWo, dBo, dWh, dBh = backpropagation(X_train, Y_train_onehot)\n",
    "    # regularization term gradients\n",
    "    dWo += lmbd * output_weights\n",
    "    dWh += lmbd * hidden_weights\n",
    "    \n",
    "    # update weights and biases\n",
    "    output_weights -= eta * dWo\n",
    "    output_bias -= eta * dBo\n",
    "    hidden_weights -= eta * dWh\n",
    "    hidden_bias -= eta * dBh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) classifying the Ising model phase using neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Critical evaluation of the various algorithms\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
